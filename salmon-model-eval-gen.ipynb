{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:38.205498Z","iopub.execute_input":"2025-10-12T15:14:38.205702Z","iopub.status.idle":"2025-10-12T15:14:39.897845Z","shell.execute_reply.started":"2025-10-12T15:14:38.205676Z","shell.execute_reply":"2025-10-12T15:14:39.897148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_personal = user_secrets.get_secret(\"personalhuggingface\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:39.89929Z","iopub.execute_input":"2025-10-12T15:14:39.899619Z","iopub.status.idle":"2025-10-12T15:14:39.950975Z","shell.execute_reply.started":"2025-10-12T15:14:39.899599Z","shell.execute_reply":"2025-10-12T15:14:39.950498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli login --token {hf_personal}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:39.951568Z","iopub.execute_input":"2025-10-12T15:14:39.951755Z","iopub.status.idle":"2025-10-12T15:14:40.675674Z","shell.execute_reply.started":"2025-10-12T15:14:39.95174Z","shell.execute_reply":"2025-10-12T15:14:40.674988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Model Relavant Data","metadata":{}},{"cell_type":"code","source":"MODEL_NAME=\"llama-mimi1.3B-greedy\" #Only for naming the output huggingface repo, no slashes allowed\n #Actual Model Path on Huggingfce\nassert MODEL_NAME!=\"temp\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:40.676649Z","iopub.execute_input":"2025-10-12T15:14:40.67685Z","iopub.status.idle":"2025-10-12T15:14:40.68111Z","shell.execute_reply.started":"2025-10-12T15:14:40.676827Z","shell.execute_reply":"2025-10-12T15:14:40.680542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Preparation: All Model Utlilty Go Here\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, MimiModel, AutoFeatureExtractor, StoppingCriteria\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\nimport re\nimport requests\nimport io\n\ndef audio_array_to_text(\n    audio_array: torch.tensor,\n    audio_tokenizer,\n    feature_extractor,\n    num_quantizers: int,\n) -> str:\n    inputs = feature_extractor(\n        raw_audio=audio_array,\n        sampling_rate=feature_extractor.sampling_rate,\n        return_tensors=\"pt\",\n    ).to(audio_tokenizer.device)\n    with torch.no_grad():\n        encoder_outputs = audio_tokenizer.encode(\n            inputs[\"input_values\"],\n            inputs[\"padding_mask\"],\n            num_quantizers=num_quantizers,\n        )\n    flatten_audio_codes = encoder_outputs.audio_codes.transpose(1, 2).reshape(-1)\n    assert flatten_audio_codes.numel() % num_quantizers == 0\n    steps = []\n    for i in range(0, flatten_audio_codes.numel(), num_quantizers):\n        group = [\n            f\"<{flatten_audio_codes[i + j].item()}_{j}>\" for j in range(num_quantizers)\n        ]\n        steps.append(group)\n\n    parts = [tok for step in steps for tok in step]\n\n    text = \"\".join(parts)\n\n    return f\"<audio>{text}</audio>\"\n\ndef text_to_audio_values(\n    text: str,\n    num_quantizers: int,\n    output_file: str,\n    audio_tokenizer,\n    feature_extractor,\n):\n    # Extract (val, idx) pairs from the <val_idx> format in the text\n    matches = re.findall(r\"<(\\d+)_(\\d+)>\", text)\n    vals = []\n    for i in range(0, len(matches), num_quantizers):\n        chunk = matches[i : i + num_quantizers]\n        if len(chunk) < num_quantizers:\n            break\n        indices = [int(idx) for _, idx in chunk]\n        if indices == list(range(num_quantizers)):\n            vals.extend(int(val) for val, _ in chunk)\n        else:\n            break\n    vals = vals[: len(vals) - len(vals) % num_quantizers]\n    \n    tensor_bt4 = torch.tensor(vals).reshape(1, -1, num_quantizers)  # (B, T, 4)\n    #print(\"text_to_audio_values\", tensor_bt4)\n    \n    tensor_b4t = tensor_bt4.transpose(1, 2).to(device)  # (B, 4, T)\n    audio_values = audio_tokenizer.decode(tensor_b4t)[0]\n    torchaudio.save(\n        output_file,\n        audio_values[0].detach().cpu(),\n        feature_extractor.sampling_rate,\n    )\n    return audio_values[0].detach().cpu()\n\n\nclass StopOnAudioEnd(StoppingCriteria):\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.target_text = \"</audio>\"\n        self.target_ids = tokenizer(\n            self.target_text, add_special_tokens=False\n        ).input_ids\n\n    def __call__(self, input_ids, scores, **kwargs):\n        if len(input_ids[0]) < len(self.target_ids):\n            return False\n        return input_ids[0][-len(self.target_ids) :].tolist() == self.target_ids\n\n\nHF_MODEL_ID=\"llm-jp/Llama-Mimi-1.3B\"\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n).eval().to(device)\n\nTOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_ID)\nNUM_QUANTIZERS = getattr(MODEL.config, \"num_quantizers\", 4)\n\n\nMIMI = MimiModel.from_pretrained(\"kyutai/mimi\").to(device).eval()\nFEATURE_EXTRACTOR = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")\n\nSTOPPING_CRITERIA = StopOnAudioEnd(TOKENIZER)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:14:40.681981Z","iopub.execute_input":"2025-10-12T15:14:40.682261Z","iopub.status.idle":"2025-10-12T15:15:49.499167Z","shell.execute_reply.started":"2025-10-12T15:14:40.682234Z","shell.execute_reply":"2025-10-12T15:15:49.498446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement the following two functions!","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef get_per_token_losses(\n    audio_sample\n): \n    #return torch.Tensor([3])\n    \"\"\"\n    Calculate all loss, given model and audio sample\n    \"\"\"\n    wav, sr = torch.Tensor(audio_sample[\"array\"]), audio_sample[\"sampling_rate\"]\n   \n    if sr != FEATURE_EXTRACTOR.sampling_rate:\n        wav = torchaudio.transforms.Resample(\n            sr, FEATURE_EXTRACTOR.sampling_rate\n        )(wav)\n        sr = FEATURE_EXTRACTOR.sampling_rate\n    \n    txt = audio_array_to_text(wav, MIMI, FEATURE_EXTRACTOR, NUM_QUANTIZERS)\n    #print(pos_txt, neg_txt)\n\n    # 3) Tokenize\n    input_ids = TOKENIZER(txt, return_tensors=\"pt\").input_ids\n\n    input_ids = input_ids.to(device)\n\n    labels = input_ids.clone()\n    labels[:, :-1] = input_ids[:, 1:].clone()\n    labels[:, -1] = -100  # don't predict the last token in this chunk\n\n    out = MODEL(input_ids=input_ids)\n    logits = out.logits  # (B, T, V)\n\n    loss_all_tokens = F.cross_entropy(\n        logits.reshape(-1, logits.size(-1)),\n        labels.reshape(-1),\n        ignore_index=-100,\n        reduction='none',\n    )\n    return loss_all_tokens\n\n@torch.no_grad()\ndef generate_continuation_audio(\n    audio_sample,\n    temperature = 0.8,\n    top_k = 1,\n    do_sample = False,\n    min_length = 144,\n    max_length = 240, #around 5s is sufficient\n): \n    wav, sr = torch.Tensor(audio_sample[\"array\"]), audio_sample[\"sampling_rate\"]\n   \n    if sr != FEATURE_EXTRACTOR.sampling_rate:\n        wav = torchaudio.transforms.Resample(\n            sr, FEATURE_EXTRACTOR.sampling_rate\n        )(wav)\n        sr = FEATURE_EXTRACTOR.sampling_rate\n    \n    txt = audio_array_to_text(wav, MIMI, FEATURE_EXTRACTOR, NUM_QUANTIZERS)\n    txt = txt.replace(\"</audio>\", \"\")\n    #print(pos_txt, neg_txt)\n\n    # 3) Tokenize\n    inputs = TOKENIZER(txt, return_tensors=\"pt\")#.input_ids\n\n    inputs = inputs.to(device)\n    \n    generated = MODEL.generate(\n        **inputs,\n        max_new_tokens=max_length,\n        min_new_tokens=min_length,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        bad_words_ids=[TOKENIZER.convert_tokens_to_ids([\"</audio>\"])] #forced continuation, do not early stop\n        #stopping_criteria=[STOPPING_CRITERIA],\n    )\n    \n    #print(\"gen shape\", generated.shape, \"iid shape\", inputs.input_ids.shape)\n    generated_text = TOKENIZER.decode(generated[0]) + \"</audio>\" #add stop token at generation end\n\n    #print(txt)\n    #print(generated_text)\n    #s()\n    \n    audio_values = text_to_audio_values(\n        generated_text,\n        num_quantizers=NUM_QUANTIZERS,\n        output_file=\"output.wav\",\n        audio_tokenizer=MIMI,\n        feature_extractor=FEATURE_EXTRACTOR,\n    )\n    return audio_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:15:49.50009Z","iopub.execute_input":"2025-10-12T15:15:49.500694Z","iopub.status.idle":"2025-10-12T15:15:49.509958Z","shell.execute_reply.started":"2025-10-12T15:15:49.500672Z","shell.execute_reply":"2025-10-12T15:15:49.509419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_features(e):\n    # audio is 16000hz, maybe resample \n    e[\"postive_sample_tokenwise_loss\"] = get_per_token_losses(e[\"positive_audio\"])\n    e[\"negative_sample_tokenwise_loss\"] = get_per_token_losses(e[\"negative_audio\"])\n    if \"consistency\" in e[\"task\"]:\n        e[\"prompt_sample_tokenwise_loss\"] = get_per_token_losses(e[\"prompt_audio\"])\n        generated_audio = generate_continuation_audio(e[\"prompt_audio\"])\n        e[\"model_generated_continuation\"] = {\"sampling_rate\": FEATURE_EXTRACTOR.sampling_rate, \"array\": generated_audio.squeeze().numpy()}\n    \n    \n    e[\"code_frame_rate\"] =  12,\n    e[\"code_depth\"] =  4\n    e[\"model_sampling_rate\"] = FEATURE_EXTRACTOR.sampling_rate,\n    e[\"ppl_sanity\"] = int((e[\"postive_sample_tokenwise_loss\"].mean() < e[\"negative_sample_tokenwise_loss\"].mean()).item()) #sanity check if number same as SALMon, would be rerun with other methods\n    print(\"sample correct:\",e[\"ppl_sanity\"])\n    return e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:15:49.511724Z","iopub.execute_input":"2025-10-12T15:15:49.5119Z","iopub.status.idle":"2025-10-12T15:15:56.300933Z","shell.execute_reply.started":"2025-10-12T15:15:49.511886Z","shell.execute_reply":"2025-10-12T15:15:56.300065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Audio, load_dataset\n\nsplts = ['bg_all_consistency', 'bg_domain_consistency', 'gender_consistency', 'rir_consistency', 'sentiment_consistency', 'speaker_consistency', 'bg_alignment', 'sentiment_alignment']\n\nfor splt in splts:\n    print(splt)\n    ds = load_dataset(\"SpeechPPL/SALMon_with_meta\", splt)\n    #ds[\"train\"] = ds[\"train\"].select([1])\n    ds = ds.map(get_model_features)\n    ds = ds.cast_column(\"model_generated_continuation\", Audio(sampling_rate=FEATURE_EXTRACTOR.sampling_rate))\n    #s()\n    print(\"Accuracy:\",sum(ds[\"train\"][\"ppl_sanity\"])/len(ds[\"train\"]))\n    \n    #break`\n    \n    \n    ds.push_to_hub(f\"SpeechPPL/SALMon_{MODEL_NAME}\", config_name=splt)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T15:15:56.301735Z","iopub.execute_input":"2025-10-12T15:15:56.302006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}